title = "Correlation Matrix",
ggtheme = theme_minimal()) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),  # Rotasi label sumbu x lebih tajam
axis.text.y = element_text(size = 8),   # Ukuran teks sumbu y
plot.title = element_text(size = 14, face = "bold"))  # Judul plot
sum(is.na(data_processed))
data_processed %>%
plot_histogram(
ggtheme = theme_lares(),
geom_histogram_args = list(bins=30,
col="black",
fill="#107896"),
ncol = 1,nrow = 1)
summary(data_processed$Pengeluaran_Per_Kapita)
data_processed %>%
ExpCatViz(clim = 10,
col = "#107896",
Page = NULL,
Flip = TRUE)
data_kur %>%
plot_bar(by = "Penerima_KUR",
ggtheme = theme_lares(),
nrow = 2,ncol = 1
)
data_processed %>%
ExpNumViz(target = "Penerima_KUR",
type = 2)
pengeluaran_per_kapita_kur <- data_processed %>%
group_by(Kode_Kabupaten) %>%
summarise(total_kapita = sum(Pengeluaran_Per_Kapita, na.rm = TRUE)) %>%
arrange(desc(total_kapita)) %>%
top_n(10, total_kapita)
View(data_processed)
head(data_processed)
write.csv(data_processed, "data_processed.csv", row.names=FALSE)
library(caret)
set.seed(14)
split_index <- createDataPartition(data_processed$Penerima_KUR, p = 0.6, list = FALSE)
data_train <- data_processed[split_index, ]
data_test <- data_processed[-split_index, ]
f_data_asli <- as.vector(table(data_processed$Penerima_KUR))
f_data_latih <- as.vector(table(data_train$Penerima_KUR))
f_data_uji <- as.vector(table(data_test$Penerima_KUR))
nama_data <- c("Data Asli", "Data Latih", "Data Uji")
partisi_data <- data.frame(nama_data, rbind(f_data_asli, f_data_latih, f_data_uji))
rownames(partisi_data) <- NULL
colnames(partisi_data) <- c("Data", "1", "5")
partisi_data
write.csv(data_train, "data_train.csv", row.names=FALSE)
write.csv(data_test, "data_test.csv", row.names=FALSE)
head(data_train)
# Library yang dibutuhkan
library(readr)
library(caret)
library(smotefamily)
library(xgboost)
library(pROC)
# Fungsi untuk melakukan penyeimbangan kelas
balance_data <- function(X, y, method = "SMOTE") {
# Konversi data ke dalam format yang sesuai
data <- cbind(X, target = y)
# Pilih metode penyeimbangan
if (method == "SMOTE") {
# SMOTE (Synthetic Minority Over-sampling Technique)
smote_result <- SMOTE(X, y)
balanced_data <- smote_result$data
} else if (method == "SVM_SMOTE") {
# SVM SMOTE
svmsmote_result <- SVM_SMOTE(X, y)
balanced_data <- svmsmote_result$data
} else if (method == "SMOTE_TOMEK") {
# SMOTE Tomek Links
smote_tomek_result <- SMOTETWICE(X, y)
balanced_data <- smote_tomek_result$data
}
# Pisahkan kembali fitur dan target
X_balanced <- balanced_data[, -ncol(balanced_data)]
y_balanced <- balanced_data[, ncol(balanced_data)]
return(list(X = X_balanced, y = y_balanced))
}
# Fungsi untuk melakukan validasi silang
cross_validate_xgboost <- function(X, y, method, n_folds = 5) {
# Inisialisasi vektor untuk menyimpan metrik
accuracy_scores <- numeric(n_folds)
precision_scores <- numeric(n_folds)
recall_scores <- numeric(n_folds)
f1_scores <- numeric(n_folds)
auc_scores <- numeric(n_folds)
# Buat fold untuk validasi silang
folds <- createFolds(y, k = n_folds)
for (i in 1:n_folds) {
# Bagi data menjadi training dan validasi
test_indices <- folds[[i]]
train_indices <- setdiff(seq_along(y), test_indices)
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[test_indices, ]
y_test <- y[test_indices]
# Lakukan penyeimbangan kelas pada data training
balanced_data <- balance_data(X_train, y_train, method)
X_train_balanced <- balanced_data$X
y_train_balanced <- balanced_data$y
# Konversi ke dalam format DMatrix XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train_balanced),
label = as.numeric(y_train_balanced) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test),
label = as.numeric(y_test) - 1)
# Fungsi untuk melakukan validasi silang
cross_validate_xgboost <- function(X, y, method, n_folds = 5) {
# Inisialisasi vektor untuk menyimpan metrik
accuracy_scores <- numeric(n_folds)
precision_scores <- numeric(n_folds)
recall_scores <- numeric(n_folds)
f1_scores <- numeric(n_folds)
auc_scores <- numeric(n_folds)
# Buat fold untuk validasi silang
folds <- createFolds(y, k = n_folds)
for (i in 1:n_folds) {
# Bagi data menjadi training dan validasi
test_indices <- folds[[i]]
train_indices <- setdiff(seq_along(y), test_indices)
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[test_indices, ]
y_test <- y[test_indices]
# Lakukan penyeimbangan kelas pada data training
balanced_data <- balance_data(X_train, y_train, method)
X_train_balanced <- balanced_data$X
y_train_balanced <- balanced_data$y
# Konversi ke dalam format DMatrix XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train_balanced),
label = as.numeric(y_train_balanced) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test),
label = as.numeric(y_test) - 1)
# Parameter XGBoost
params <- list(
objective = "binary:logistic",
eta = 0.1,
max_depth = 6,
eval_metric = "auc"
)
# Fungsi untuk melakukan penyeimbangan kelas
balance_data <- function(X, y, method = "SMOTE") {
# Konversi data ke dalam format yang sesuai
data <- cbind(X, target = y)
# Pilih metode penyeimbangan
if (method == "SMOTE") {
# SMOTE (Synthetic Minority Over-sampling Technique)
smote_result <- SMOTE(X, y)
balanced_data <- smote_result$data
} else if (method == "SVM_SMOTE") {
# SVM SMOTE
svmsmote_result <- SVM_SMOTE(X, y)
balanced_data <- svmsmote_result$data
} else if (method == "SMOTE_TOMEK") {
# SMOTE Tomek Links
smote_tomek_result <- SMOTETWICE(X, y)
balanced_data <- smote_tomek_result$data
}
# Pisahkan kembali fitur dan target
X_balanced <- balanced_data[, -ncol(balanced_data)]
y_balanced <- balanced_data[, ncol(balanced_data)]
return(list(X = X_balanced, y = y_balanced))
}
# Fungsi untuk melakukan validasi silang
cross_validate_xgboost <- function(X, y, method, n_folds = 5) {
# Inisialisasi vektor untuk menyimpan metrik
accuracy_scores <- numeric(n_folds)
precision_scores <- numeric(n_folds)
recall_scores <- numeric(n_folds)
f1_scores <- numeric(n_folds)
auc_scores <- numeric(n_folds)
# Buat fold untuk validasi silang
folds <- createFolds(y, k = n_folds)
for (i in 1:n_folds) {
# Bagi data menjadi training dan validasi
test_indices <- folds[[i]]
train_indices <- setdiff(seq_along(y), test_indices)
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[test_indices, ]
y_test <- y[test_indices]
# Lakukan penyeimbangan kelas pada data training
balanced_data <- balance_data(X_train, y_train, method)
X_train_balanced <- balanced_data$X
y_train_balanced <- balanced_data$y
# Konversi ke dalam format DMatrix XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train_balanced),
label = as.numeric(y_train_balanced) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test),
label = as.numeric(y_test) - 1)
# Parameter XGBoost
params <- list(
objective = "binary:logistic",
eta = 0.1,
max_depth = 6,
eval_metric = "auc"
)
library(readr)
library(caret)
library(smotefamily)
library(xgboost)
library(pROC)
# Fungsi untuk melakukan penyeimbangan kelas
balance_data <- function(X, y, method = "SMOTE") {
# Konversi data ke dalam format yang sesuai
data <- data.frame(X, target = y)
# Pilih metode penyeimbangan
if (method == "SMOTE") {
# SMOTE (Synthetic Minority Over-sampling Technique)
smote_result <- SMOTE(X, y)
balanced_data <- smote_result$data
} else if (method == "SVM_SMOTE") {
# SVM SMOTE
svmsmote_result <- SVM_SMOTE(X, y)
balanced_data <- svmsmote_result$data
} else if (method == "SMOTE_TOMEK") {
# SMOTE Tomek Links
smote_tomek_result <- SMOTETWICE(X, y)
balanced_data <- smote_tomek_result$data
}
# Pisahkan kembali fitur dan target
X_balanced <- balanced_data[, -ncol(balanced_data)]
y_balanced <- balanced_data[, ncol(balanced_data)]
return(list(X = X_balanced, y = y_balanced))
}
Fungsi untuk melakukan validasi silang XGBoost
# Fungsi untuk melakukan validasi silang XGBoost
cross_validate_xgboost <- function(X, y, method, n_folds = 5) {
# Inisialisasi vektor untuk menyimpan metrik
accuracy_scores <- numeric(n_folds)
precision_scores <- numeric(n_folds)
recall_scores <- numeric(n_folds)
f1_scores <- numeric(n_folds)
auc_scores <- numeric(n_folds)
# Buat fold untuk validasi silang
folds <- createFolds(y, k = n_folds)
for (i in 1:n_folds) {
# Bagi data menjadi training dan validasi
test_indices <- folds[[i]]
train_indices <- setdiff(seq_along(y), test_indices)
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[test_indices, ]
y_test <- y[test_indices]
# Lakukan penyeimbangan kelas pada data training
balanced_data <- balance_data(X_train, y_train, method)
X_train_balanced <- balanced_data$X
y_train_balanced <- balanced_data$y
# Konversi ke dalam format DMatrix XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train_balanced),
label = as.numeric(y_train_balanced) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test),
label = as.numeric(y_test) - 1)
# Parameter XGBoost
params <- list(
objective = "binary:logistic",
eta = 0.1,
max_depth = 6,
eval_metric = "auc"
)
library(caret)
set.seed(14)
split_index <- createDataPartition(data_processed$Penerima_KUR, p = 0.7, list = FALSE)
data_train <- data_processed[split_index, ]
data_test <- data_processed[-split_index, ]
f_data_asli <- as.vector(table(data_processed$Penerima_KUR))
f_data_latih <- as.vector(table(data_train$Penerima_KUR))
f_data_uji <- as.vector(table(data_test$Penerima_KUR))
nama_data <- c("Data Asli", "Data Latih", "Data Uji")
partisi_data <- data.frame(nama_data, rbind(f_data_asli, f_data_latih, f_data_uji))
rownames(partisi_data) <- NULL
colnames(partisi_data) <- c("Data", "1", "5")
partisi_data
write.csv(data_train, "data_train.csv", row.names=FALSE)
write.csv(data_test, "data_test.csv", row.names=FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(lares)
library(tidyverse)
library(tidymodels)
library(themis)
library(tidyposterior)
library(SmartEDA)
library(DataExplorer)
library(skimr)
library(ggpubr)
library(workflowsets)
library(nnet)
data_kur <- read.csv("data_fix_new.csv")
head(data_kur)
data_summary_class <- data_kur %>%
group_by(Penerima_KUR) %>%
summarise(count = n()) %>%
mutate(percentage = count / sum(count) * 100)
ggplot(data_summary_class, aes(x = "", y = count, fill = as.factor(Penerima_KUR))) +
geom_bar(stat = "identity", width = 1) +
coord_polar(theta = "y") +
geom_text(aes(label = paste0(count, " (", round(percentage, 1), "%)")),
position = position_stack(vjust = 0.5),
color = "black") +
labs(title = "Distribusi Penerima KUR Di 5 Kabupaten/Kota Provinsi Jawa Barat",
fill = "Status Penerima KUR") +
scale_fill_manual(values = c("1" = "green", "5" = "red"),
labels = c("Ya", "Tidak")) +
theme_minimal() +
theme(
axis.text.x = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.y = element_blank()
)
data_processed <- data_kur %>%
select(
Kode_Kabupaten,
Penerima_KUR,
Klasifikasi_Perkotaan_Perdesaan,
Status_Kepemilikan_Rumah,
Memiliki_Rumah_Lain,
Bahan_Atap,
Bahan_Dinding,
Bahan_Lantai,
Memiliki_Fasilitas_BAB,
Menerima_Kredit_Lain,
Memiliki_Gas_Min5kg,
Memiliki_Kulkas,
Memiliki_AC,
Memiliki_Motor,
Memiliki_Mobil,
Memiliki_Lahan,
Pengeluaran_Per_Kapita,
Luas_Lantai_Bangunan,
Jumlah_ART
) %>%
mutate(
across(
c(
Penerima_KUR,
Kode_Kabupaten,
Klasifikasi_Perkotaan_Perdesaan,
Status_Kepemilikan_Rumah,
Memiliki_Rumah_Lain,
Bahan_Atap,
Bahan_Dinding,
Bahan_Lantai,
Memiliki_Fasilitas_BAB,
Menerima_Kredit_Lain,
Memiliki_Gas_Min5kg,
Memiliki_Kulkas,
Memiliki_AC,
Memiliki_Motor,
Memiliki_Mobil,
Memiliki_Lahan
),
as.factor
),
Pengeluaran_Per_Kapita = as.numeric(Pengeluaran_Per_Kapita),
Luas_Lantai_Bangunan = as.numeric(Luas_Lantai_Bangunan),
Jumlah_ART = as.numeric(Jumlah_ART)
)
head(data_processed)
plot_intro(data = data_processed,
ggtheme = theme_classic(),
theme_config = list(axis.line=element_blank(),
axis.ticks=element_blank(),
axis.text.x=element_blank(),
axis.title=element_blank()
)
)
# Jika belum terpasang
# install.packages("ggcorrplot")
library(ggcorrplot)
# Membuat matriks korelasi hanya dari kolom numerik
numeric_data <- data_kur[, sapply(data_kur, is.numeric)]
cor_matrix <- cor(numeric_data)
# Plot korelasi dengan pengaturan tambahan untuk meningkatkan keterbacaan
# Plot korelasi dengan pengaturan tambahan
ggcorrplot(cor_matrix,
lab = TRUE,           # Menambahkan nilai korelasi
lab_size = 2.9,         # Ukuran label lebih kecil
method = "square",    # Tipe plot
colors = c("blue", "white", "red"), # Skala warna
outline.color = "gray",
type = "upper",       # Menampilkan setengah matriks
title = "Correlation Matrix",
ggtheme = theme_minimal()) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),  # Rotasi label sumbu x lebih tajam
axis.text.y = element_text(size = 8),   # Ukuran teks sumbu y
plot.title = element_text(size = 14, face = "bold"))  # Judul plot
sum(is.na(data_processed))
data_processed %>%
plot_histogram(
ggtheme = theme_lares(),
geom_histogram_args = list(bins=30,
col="black",
fill="#107896"),
ncol = 1,nrow = 1)
data_processed %>%
ExpCatViz(clim = 10,
col = "#107896",
Page = NULL,
Flip = TRUE)
summary(data_processed$Pengeluaran_Per_Kapita)
data_kur %>%
plot_bar(by = "Penerima_KUR",
ggtheme = theme_lares(),
nrow = 2,ncol = 1
)
data_processed %>%
ExpNumViz(target = "Penerima_KUR",
type = 2)
pengeluaran_per_kapita_kur <- data_processed %>%
group_by(Kode_Kabupaten) %>%
summarise(total_kapita = sum(Pengeluaran_Per_Kapita, na.rm = TRUE)) %>%
arrange(desc(total_kapita)) %>%
top_n(10, total_kapita)
# Visualisasikan
ggplot(pengeluaran_per_kapita_kur, aes(x = reorder(Kode_Kabupaten, total_kapita), y = total_kapita)) +
geom_bar(stat = "identity", fill = "darkgreen") +
labs(title = "Sebaran Pengeluaran per Kapita di 5 Kabupaten",
x = "Kabupaten", y = "Total Keluaran Kapitan") +
coord_flip() +
theme_minimal()
View(pengeluaran_per_kapita_kur)
source("~/Documents/Kuliah_Statistika/Eksplorasi dan Visualisasi Data/shiny-dashboardvisdat/shiny-visdat/data_try.R", echo=TRUE)
source("~/Documents/Kuliah_Statistika/Eksplorasi dan Visualisasi Data/shiny-dashboardvisdat/shiny-visdat/data_try.R", echo=TRUE)
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
View(data_jbdtk)
runApp()
library(tidyverse)
library(ggplot2)
library(caret)
library(corrplot)
library(leaflet)
library(plotly)
library(readr)
library(dplyr)
library(OneR)
library(e1071)
library(tree)
library(party)
library(rpart)
library(rmarkdown)
library(tidyverse)
library(ggplot2)
library(caret)
library(corrplot)
library(leaflet)
library(plotly)
library(readr)
library(dplyr)
library(OneR)
library(e1071)
library(tree)
library(party)
library(rpart)
library(rmarkdown)
dt_credit <- read.csv("credit_risk_dataset.csv")
glimpse(dt_credit)
nrow(dt_credit)
sum(is.na(dt_credit))
dt_credit <- na.omit(dt_credit)
sum(is.na(dt_credit))
sapply(dt_credit, function(x)sum(is.na(x)/nrow(dt_credit)))
colSums(is.na(dt_credit))
str(dt_credit)
summary(dt_credit)
library(visdat)
vis_dat(dt_credit, warn_large_data = F)
DataExplorer::plot_intro(dt_credit)
library(corrplot)
numeric_dt <- dt_credit[, sapply(dt_credit, is.numeric)]
mtr <- cor(numeric_dt)
# Menyiapkan skala warna
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
# Menampilkan matriks korelasi dengan corrplot
corrplot(mtr, method="color", col=col(200),
addCoef.col = "black",
tl.col="black", tl.srt=45,
diag=FALSE)
dt_credit <- dt_credit %>%
mutate(experience = person_age - cb_person_cred_hist_length)
dt_credit
dt_credit_final <- dt_credit
dt_credit_final$person_age <- NULL
dt_credit_final$cb_person_cred_hist_length <- NULL
library(corrplot)
numeric_dt <- dt_credit_final[, sapply(dt_credit_final, is.numeric)]
mtr <- cor(numeric_dt)
# Menyiapkan skala warna
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
# Menampilkan matriks korelasi dengan corrplot
corrplot(mtr, method="color", col=col(200),
addCoef.col = "black",
tl.col="black", tl.srt=45,
diag=FALSE)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
